{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for fetching relevant metadata\n",
    "LitCovid aleady classifies the majority of its records into a few broad categories. Here we leverage those categories to build a broad classifier to classify preprints based on their abstracts. If more detailed classification is desired, we can run a 2-step classifier (first to broadly classify, then to narrow down)\n",
    "\n",
    "We start with a dump of the classifications using the LitCovidTopics parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the name, abstract for the pmids\n",
    "#### Note, I've tried batches of 1000, and the post request has failed, so this uses a batch size that's less likely to fail\n",
    "def batch_fetch_meta(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pd.DataFrame(columns = ['_id','abstract','name','description'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract,description'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pd.read_json(r.text)\n",
    "            checkcols = rawresult.columns\n",
    "            if (('description' not in checkcols) and ('abstract' in checkcols)):\n",
    "                rawresult['description']=\" \"\n",
    "            elif (('description' in checkcols) and ('abstract' not in checkcols)):\n",
    "                rawresult['abstract']=\" \"\n",
    "            elif (('description' not in checkcols) and ('abstract' not in checkcols)):\n",
    "                rawresult['abstract']=\" \"\n",
    "                rawresult['description']=\" \"\n",
    "            cleanresult = rawresult[['_id','name','abstract','description']].loc[rawresult['_score']==1].fillna(\" \").copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            textdf = pd.concat((textdf,cleanresult))\n",
    "        i=i+1\n",
    "    return(textdf)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for transforming the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge text from the name, abstract, and description\n",
    "#### Clean up up the text\n",
    "\n",
    "def merge_texts(df):\n",
    "    df.fillna('',inplace=True)\n",
    "    df['text'] = df['name'].astype(str).str.cat(df['abstract'].astype(str).str.cat(df['description'],sep=' '),sep=' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\W', ' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ')\n",
    "    df['text'] = df['text'].str.replace(r'\\^[a-zA-Z]\\s+', ' ')\n",
    "    df['text'] = df['text'].str.lower()   \n",
    "    return(df)\n",
    "\n",
    "\n",
    "def fetch_categorized_data(df):\n",
    "    alldata = pd.DataFrame(columns=['_id','name','abstract','description','text','topicCategory'])\n",
    "    breakdown = df.groupby('topicCategory').size().reset_index(name='counts')\n",
    "    for eachtopic in breakdown['topicCategory'].tolist():\n",
    "        tmpids = df['_id'].loc[df['topicCategory']==eachtopic]\n",
    "        tmptxtdf = batch_fetch_meta(tmpids)\n",
    "        tmptxtdf = merge_texts(tmptxtdf)\n",
    "        tmptxtdf['topicCategory']=eachtopic\n",
    "        alldata = pd.concat((alldata,tmptxtdf),ignore_index=True)\n",
    "    return(alldata)\n",
    "\n",
    "\n",
    "def generate_training_df(df,category):\n",
    "    positiveids = df['_id'].loc[df['topicCategory']==category].tolist()\n",
    "    training_set_pos = df[['_id','text']].loc[df['topicCategory']==category].copy()\n",
    "    training_set_pos['target']='in category'\n",
    "    max_negs = len(positiveids)\n",
    "    if len(positiveids)<len(df.loc[~df['_id'].isin(positiveids)]):\n",
    "        training_set_neg = df[['_id','text']].loc[~df['_id'].isin(positiveids)].sample(n=max_negs).copy()\n",
    "    else:\n",
    "        training_set_neg = df[['_id','text']].loc[~df['_id'].isin(positiveids)].copy()\n",
    "    training_set_neg['target']='not in category'\n",
    "    training_set = pd.concat((training_set_pos,training_set_neg),ignore_index=True)\n",
    "    return(training_set)\n",
    "\n",
    "\n",
    "#### Note that this function is to clean up the classification predictions and format it as annotations\n",
    "def clean_results(allresults):\n",
    "    allresults.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    counts = allresults.groupby('_id').size().reset_index(name='counts')\n",
    "    duplicates = counts.loc[counts['counts']>1]\n",
    "    singles = counts.loc[counts['counts']==1]\n",
    "    dupids = duplicates['_id'].unique().tolist()\n",
    "    tmplist = []\n",
    "    for eachid in dupids:\n",
    "        catlist = allresults['topicCategory'].loc[allresults['_id']==eachid].tolist()\n",
    "        tmplist.append({'_id':eachid,'topicCategory':catlist})\n",
    "    tmpdf = pd.DataFrame(tmplist)  \n",
    "    tmpsingledf = allresults[['_id','topicCategory']].loc[allresults['_id'].isin(singles['_id'].tolist())]\n",
    "    idlist = tmpsingledf['_id'].tolist()\n",
    "    catlist = tmpsingledf['topicCategory'].tolist()\n",
    "    cattycat = [[x] for x in catlist]\n",
    "    list_of_tuples = list(zip(idlist,cattycat))\n",
    "    singledf = pd.DataFrame(list_of_tuples, columns = ['_id', 'topicCategory']) \n",
    "    cleanresults = pd.concat((tmpdf,singledf),ignore_index=True)\n",
    "    return(cleanresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_classify(classifier,training_set,X,i=0):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, training_set.target, test_size=0.2, random_state=i)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cmresult = confusion_matrix(y_test,y_pred)\n",
    "    report = pd.DataFrame(classification_report(y_test,y_pred,output_dict=True))\n",
    "    probs = classifier.predict_proba(X_test)\n",
    "    probs = probs[:, 1]\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    return(cmresult,report,auc)\n",
    "\n",
    "\n",
    "def vectorize_text(training_set):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(training_set['text'])\n",
    "    features = vectorizer.get_feature_names()\n",
    "    return(X)\n",
    "\n",
    "\n",
    "def generate_vectorizer(training_set,category):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(training_set['text'])\n",
    "    features = vectorizer.get_feature_names()\n",
    "    vectorizerfile = os.path.join(MODELPATH,\"vectorizer_\"+category+\".pickle\")\n",
    "    xfile = os.path.join(MODELPATH,\"X_\"+category+\".pickle\")\n",
    "    pickle.dump(vectorizer, open(vectorizerfile, \"wb\"))\n",
    "    pickle.dump(X, open(xfile, \"wb\"))\n",
    "    return(X)\n",
    "\n",
    "\n",
    "def save_model(classifier,classname,category):\n",
    "    filename = os.path.join(MODELPATH,classname+\"_\"+category+\".sav\")\n",
    "    pickle.dump(classifier, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for testing different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the classifiers\n",
    "\n",
    "def load_classifiers(classifierset_type):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn import tree\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    all_available = {\n",
    "        'Random Forest':RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "        'MultinomialNB':MultinomialNB(),\n",
    "        'Neural Net':MLPClassifier(alpha=1, max_iter=1000),\n",
    "        'Decision Tree':tree.DecisionTreeClassifier(max_depth=5),\n",
    "        'Nearest Neighbor':KNeighborsClassifier(3),\n",
    "        'AdaBoost':AdaBoostClassifier(),\n",
    "        'Logistic Regression':LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')}\n",
    "    best = {\n",
    "        'Random Forest':RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "        'MultinomialNB':MultinomialNB(),\n",
    "        'Logistic Regression':LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr')}\n",
    "    if classifierset_type=='best':\n",
    "        return(best)\n",
    "    else:\n",
    "        return(all_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading preprint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the size of the source (to make it easy to figure out when to stop scrolling)\n",
    "def fetch_src_size(source):\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_source_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "#### Pull ids from the major publication sources (litcovid, medrxiv,biorxiv)\n",
    "def get_preprint_ids():\n",
    "    biorxiv_ids = get_source_ids(\"bioRxiv\")\n",
    "    medrxiv_ids = get_source_ids(\"medRxiv\")\n",
    "    preprint_ids = list(set(medrxiv_ids).union(set(biorxiv_ids)))\n",
    "    return(preprint_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for classifying all publications (not yet classified)\n",
    "\n",
    "def get_pub_ids(sourceset):\n",
    "    pub_srcs = {\"preprint\":[\"bioRxiv\",\"medRxiv\"],\"litcovid\":[\"litcovid\"],\n",
    "                \"other\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\"],\n",
    "                \"all\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\",\n",
    "                       \"bioRxiv\",\"medRxiv\",\"litcovid\"]}\n",
    "    sourcelist = pub_srcs[sourceset]\n",
    "    allids = []\n",
    "    for eachsource in sourcelist:\n",
    "        sourceids = get_source_ids(eachsource)\n",
    "        allids = list(set(allids).union(set(sourceids)))\n",
    "    return(allids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for utilizing pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer(category):\n",
    "    vectorizerfile = os.path.join(MODELPATH,\"vectorizer_\"+category+\".pickle\")\n",
    "    vectorizer = pickle.load(open(vectorizerfile,'rb'))\n",
    "    return(vectorizer)\n",
    "\n",
    "def predict_class(topiclist,classifierlist,df):\n",
    "    labels = df['_id']\n",
    "    for eachtopic in topiclist:\n",
    "        vectorizer = load_vectorizer(eachtopic)\n",
    "        M = vectorizer.transform(df['text'])\n",
    "        for eachclassifier in classifierlist:\n",
    "            classifierfile = os.path.join(MODELPATH, eachclassifier+\"_\"+eachtopic+'.sav')\n",
    "            classifier = pickle.load(open(classifierfile, 'rb'))\n",
    "            prediction = classifier.predict(M)\n",
    "            list_of_tuples = list(zip(labels,prediction))\n",
    "            predictiondf = pd.DataFrame(list_of_tuples, columns = ['_id', 'prediction'])\n",
    "            predictiondf['topicCategory']=eachtopic\n",
    "            predictiondf['classifier']=eachclassifier\n",
    "            predictiondf.to_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclassifier+'.tsv'),sep='\\t',header=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for evaluating classification predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def merge_predictions(topiclist,classifierlist,agreetype=\\'perfect\\'):\\n    totalagree = pd.DataFrame(columns=[\\'_id\\',\\'topicCategory\\'])\\n    for eachtopic in topiclist:\\n        agreement = filter_agreement(topiclist,classifierlist,agreetype=\\'perfect\\')\\n        totalagree = pd.concat((totalagree,agreement),ignore_index=True)\\n    totalagree.drop_duplicates(inplace=True,keep=\"first\")\\n    return(totalagree)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_agreement(eachtopic,classifierlist,PREDICTPATH):\n",
    "    agreement = pd.DataFrame(columns=['_id','topicCategory','pos_pred_count','pos_pred_algorithms'])\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreecounts = posresults.groupby('_id').size().reset_index(name='counts')\n",
    "    no_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==1].tolist())].copy()\n",
    "    no_agree.rename(columns={'classifier':'pos_pred_algorithms'},inplace=True)\n",
    "    no_agree['pos_pred_count']=1\n",
    "    no_agree.drop('prediction',axis=1,inplace=True)\n",
    "    perfect_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==len(classifierlist)].tolist())].copy()\n",
    "    perfect_agree['pos_pred_count']=len(classifierlist)\n",
    "    perfect_agree['pos_pred_algorithms']=str(classifierlist)\n",
    "    perfect_agree.drop(['prediction','classifier'],axis=1,inplace=True)\n",
    "    perfect_agree.drop_duplicates('_id',keep='first',inplace=True)\n",
    "    partialcountids = agreecounts['_id'].loc[((agreecounts['counts']>1)&\n",
    "                                          (agreecounts['counts']<len(classifierlist)))].tolist()\n",
    "    tmplist = []\n",
    "    for eachid in list(set(partialcountids)):\n",
    "        tmpdf = posresults.loc[posresults['_id']==eachid]\n",
    "        tmpdict = {'_id':eachid,'topicCategory':eachtopic,'pos_pred_count':len(tmpdf),\n",
    "                   'pos_pred_algorithms':str(tmpdf['classifier'].tolist())}\n",
    "        tmplist.append(tmpdict)\n",
    "    partial_agree = pd.DataFrame(tmplist)    \n",
    "    agreement = pd.concat((agreement,no_agree,partial_agree,perfect_agree),ignore_index=True)\n",
    "    return(agreement)\n",
    "\n",
    "def filter_agreement(topiclist,classifierlist,agreetype='perfect'):\n",
    "    allagreement = pd.DataFrame(columns=['_id','topicCategory','pos_pred_count','pos_pred_algorithms'])\n",
    "    for eachtopic in topiclist:\n",
    "        agreement = get_agreement(eachtopic,classifierlist,PREDICTPATH)\n",
    "        allagreement = pd.concat((allagreement,agreement),ignore_index=True)\n",
    "    if agreetype=='perfect':\n",
    "        filtered_agreement = allagreement[['_id','topicCategory']].loc[allagreement['pos_pred_count']==len(classifierlist)].copy()\n",
    "    elif agreetype=='None':\n",
    "        filtered_agreement = allagreement[['_id','topicCategory']].loc[allagreement['pos_pred_count']==1].copy()\n",
    "    else:\n",
    "        partialcountids = allagreement['_id'].loc[((allagreement['pos_pred_count']>1)&\n",
    "                                          (allagreement['pos_pred_count']<len(classifierlist)))].tolist()\n",
    "        filtered_agreement = allagreement[['_id','topicCategory']].loc[allagreement['_id'].isin(partialcountids)].copy()\n",
    "    return(filtered_agreement)\n",
    "\n",
    "\n",
    "def merge_predictions(topiclist,classifierlist,agreetype='perfect'):\n",
    "    agreement = filter_agreement(topiclist,classifierlist,agreetype='perfect')\n",
    "    agreement.drop_duplicates(inplace=True,keep=\"first\")\n",
    "    return(agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Functions\n",
    "Primary functions for performing testing, training, classification prediction, and providing the results as annotations\n",
    "0. Run algorithm tests against the data and report results\n",
    "1. Train the classifiers on the LitCovid data\n",
    "2. Apply the classifiers to the preprint data\n",
    "3. Clean up the results and serve them up as annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def load_annotations(topicsdf,classifiers):\\n    topiclist = topicsdf['topicCategory'].unique().tolist()\\n    classify_pubs(topiclist,classifiers)\\n    classifierlist = classifiers.keys()\\n    total_agree = merge_predictions(topiclist,classifierlist,agreetype='perfect')\\n    cleanresults = clean_results(total_agree)\\n    originalresults = clean_results(topicsdf)\\n    allresults = pd.concat((originalresults,cleanresults),ignore_index=True)\\n    allresults.to_csv(os.path.join(RESULTPATH,'predictions.tsv'),sep='\\t',header=0)\\n    #allresults.to_json(os.path.join(RESULTPATH,'predictions.json'), orient='records')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_test(topicsdf,classifierset_type='best',export_report=False):\n",
    "    classifiers = load_classifiers(classifierset_type)\n",
    "    fetchstarttime = datetime.now()\n",
    "    print(\"fetching the abstracts: \", fetchstarttime)\n",
    "    alldata = fetch_categorized_data(topicsdf)\n",
    "    fetchtime = datetime.now()-fetchstarttime\n",
    "    print(\"fetching complete: \",fetchtime)\n",
    "    breakdown = alldata.groupby('topicCategory').size().reset_index(name='counts')\n",
    "    testresults = []\n",
    "    for eachtopic in breakdown['topicCategory'].tolist():\n",
    "        print(\"now testing: \",eachtopic,datetime.now())\n",
    "        training_set = generate_training_df(alldata,eachtopic)\n",
    "        X = vectorize_text(training_set)\n",
    "        for classifier in classifiers.keys():\n",
    "            i=0\n",
    "            while i<5:\n",
    "                timestart = datetime.now()\n",
    "                cmresult,report,auc = train_test_classify(classifiers[classifier],training_set,X,i)\n",
    "                runtime = datetime.now() - timestart\n",
    "                testresults.append({'topicCategory':eachtopic,'set size':len(training_set),'classifier':classifier,\n",
    "                                    'runtime':runtime,'auc':auc,'report':report,'matrix':cmresult,'i':i})\n",
    "                i=i+1\n",
    "    testresultsdf = pd.DataFrame(testresults)\n",
    "    if export_report==True:\n",
    "        testresultsdf.to_csv(os.path.join(RESULTPATH,'in_depth_classifier_test.tsv'),sep='\\t',header=True)\n",
    "    return(testresultsdf)\n",
    "\n",
    "\n",
    "def generate_models(topicsdf,classifiers):\n",
    "    alldata = fetch_categorized_data(topicsdf)\n",
    "    breakdown = alldata.groupby('topicCategory').size().reset_index(name='counts')\n",
    "\n",
    "    for eachtopic in breakdown['topicCategory'].tolist():\n",
    "        trainingset = generate_training_df(alldata,eachtopic)\n",
    "        X = generate_vectorizer(trainingset,eachtopic)\n",
    "        for eachclassifier in classifiers.keys():\n",
    "            classifier=classifiers[eachclassifier]\n",
    "            classifier.fit(X, trainingset.target)\n",
    "            save_model(classifier,eachclassifier,eachtopic)  \n",
    "\n",
    "            \n",
    "def classify_preprints(topiclist,classifiers):\n",
    "    preprint_ids = get_pub_ids(sourceset=\"preprint\")\n",
    "    preprintdf = batch_fetch_meta(preprint_ids)\n",
    "    preprintdata = merge_texts(preprintdf)    \n",
    "    classifierlist = classifiers.keys()\n",
    "    predict_class(topiclist,classifierlist,preprintdata)\n",
    "\n",
    "    \n",
    "def classify_pubs(topiclist,classifiers):\n",
    "    all_ids = get_pub_ids(sourceset=\"other\")\n",
    "    alldf = batch_fetch_meta(all_ids)\n",
    "    alldata = merge_texts(alldf)    \n",
    "    classifierlist = classifiers.keys()\n",
    "    predict_class(topiclist,classifierlist,alldata)\n",
    "    \n",
    "    \n",
    "def load_annotations(topicsdf,classifiers):\n",
    "    topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "    classify_pubs(topiclist,classifiers)\n",
    "    classifierlist = classifiers.keys()\n",
    "    total_agree = merge_predictions(topiclist,classifierlist,agreetype='perfect')\n",
    "    allresults = pd.concat((total_agree,topicsdf),ignore_index=True)\n",
    "    cleanresults = clean_results(allresults)\n",
    "    cleanresults.to_csv(os.path.join(RESULTPATH,'predictions.tsv'),sep='\\t',header=0)\n",
    "    cleanresults.to_json(os.path.join(RESULTPATH,'predictions.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Script Run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idlist = [\"zenodo.3766145\",\"figshare12864673\",\"2020.12.06.413682\",\"pmid33677050\"]\n",
    "idlist = get_pub_ids(sourceset=\"other\")\n",
    "runs = round((len(idlist))/100,0)\n",
    "i=12 \n",
    "separator = ','\n",
    "## Create dummy dataframe to store the meta data\n",
    "textdf = pd.DataFrame(columns = ['_id','abstract','name','description'])\n",
    "while i < runs+1:\n",
    "    if len(idlist)<100:\n",
    "        sample = idlist\n",
    "    elif i == 0:\n",
    "        sample = idlist[i:(i+1)*100]\n",
    "    elif i == runs:\n",
    "        sample = idlist[i*100:len(idlist)]\n",
    "    else:\n",
    "        sample = idlist[i*100:(i+1)*100]\n",
    "    sample_ids = separator.join(sample)\n",
    "    print(sample_ids)\n",
    "    ## Get the text-based metadata (abstract, title) and save it\n",
    "    r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract,description'})\n",
    "    if r.status_code == 200:\n",
    "        rawresult = pd.read_json(r.text)\n",
    "        checkcols = rawresult.columns\n",
    "        if (('description' not in checkcols) and ('abstract' in checkcols)):\n",
    "            rawresult['description']=\" \"\n",
    "        elif (('description' in checkcols) and ('abstract' not in checkcols)):\n",
    "            rawresult['abstract']=\" \"\n",
    "        elif (('description' not in checkcols) and ('abstract' not in checkcols)):\n",
    "            rawresult['abstract']=\" \"\n",
    "            rawresult['description']=\" \"\n",
    "        cleanresult = rawresult[['_id','name','abstract','description']].loc[rawresult['_score']==1].fillna(\" \").copy()\n",
    "        cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "        textdf = pd.concat((textdf,cleanresult))\n",
    "    i=i+1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating models:  2021-04-16 09:39:15.692008\n",
      "2:39:55.335474\n",
      "load annotations:  2021-04-16 12:19:11.027482\n",
      "1:26:46.556467\n"
     ]
    }
   ],
   "source": [
    "## Pull the classifications from the LitCovidTopics parser\n",
    "DATAPATH = 'data/'\n",
    "RESULTPATH = 'results/'\n",
    "MODELPATH = 'models/'\n",
    "PREDICTPATH = 'predictions/'\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(RESULTPATH,'training_ids.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "\"\"\"\n",
    "runtimesinfo=[]\n",
    "timestart = datetime.now()\n",
    "print('testing start: ',timestart)\n",
    "testresultsdf = run_test(topicsdf,classifierset_type='best',export_report=True)\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'run_test()'})\n",
    "\n",
    "timestart = datetime.now()\n",
    "print('creating models: ',timestart)\n",
    "classifiers = load_classifiers('best')\n",
    "generate_models(topicsdf,classifiers)\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'generate_models()'})\n",
    "\n",
    "runtimesinfo=[]\n",
    "timestart = datetime.now()\n",
    "print('load_classifiers: ',timestart)\n",
    "classifiers = load_classifiers('best')\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'generate_models()'})\n",
    "\n",
    "timestart = datetime.now()\n",
    "print('classifying non-litcovid: ',timestart)\n",
    "classify_pubs(topiclist,classifiers)\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'classify_preprints()'})\n",
    "\n",
    "timestart = datetime.now()\n",
    "print('merging results: ',timestart)\n",
    "classifierlist = classifiers.keys()\n",
    "total_agree = merge_predictions(topiclist,classifierlist,agreetype='perfect')\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'classify_preprints()'})\n",
    "\"\"\"\n",
    "runtimesinfo=[]\n",
    "timestart = datetime.now()\n",
    "print('creating models: ',timestart)\n",
    "classifiers = load_classifiers('best')\n",
    "generate_models(topicsdf,classifiers)\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'generate_models()'})\n",
    "\n",
    "timestart = datetime.now()\n",
    "print('load annotations: ',timestart)\n",
    "load_annotations(topicsdf,classifiers)\n",
    "runtime = datetime.now()-timestart\n",
    "print(runtime)\n",
    "runtimesinfo.append({'starttime':timestart,'runtime':runtime,'function':'load_annotations()'})\n",
    "\n",
    "runtimesdf = pd.DataFrame(runtimesinfo)\n",
    "runtimesdf.to_csv(os.path.join(RESULTPATH,'runtimes.tsv'),sep='\\t',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To do\n",
    "## Check quality of Information sciences training data (because some of the results look iffy)\n",
    "## Time the clean process and the merge process\n",
    "## Verify the load annotations function works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allresults = pd.concat((total_agree,topicsdf),ignore_index=True)\n",
    "print(allresults)\n",
    "cleanresults = clean_results(allresults)\n",
    "print(cleanresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check merging of predicted classes with litcovid and other training classes\n",
    "\n",
    "allresults = pd.concat((total_agree,topicsdf),ignore_index=True)\n",
    "print(len(allresults))\n",
    "cleanresults = clean_results(allresults)\n",
    "print(len(cleanresults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_agree.head(n=2))\n",
    "print(len(total_agree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifiers = load_classifiers('best')\n",
    "classifierlist = classifiers.keys()\n",
    "total_agree = merge_predictions(topiclist,classifierlist,agreetype='perfect')\n",
    "cleanresults = clean_results(total_agree)\n",
    "cleanresults.to_csv(os.path.join(RESULTPATH,'predictions.tsv'),sep='\\t',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanresults.to_json(os.path.join(RESULTPATH,'predictions.json'), orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pull the classifications from the LitCovidTopics parser\n",
    "DATAPATH = 'data/'\n",
    "topicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "topicsdf = read_csv(topicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "RESULTPATH = 'results/'\n",
    "MODELPATH = 'models/'\n",
    "PREDICTPATH = 'predictions/'\n",
    "\n",
    "## Reserve records which span multiple categories for test since they are ambiguous and could confuse the training\n",
    "frequencies = topicsdf.groupby('_id').size().reset_index(name='counts')\n",
    "ambiguous = frequencies['_id'].loc[frequencies['counts']>1].tolist()\n",
    "unambiguous = topicsdf.loc[~topicsdf['_id'].isin(ambiguous)]\n",
    "print(len(ambiguous),len(unambiguous))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = fetch_categorized_data(unambiguous)\n",
    "breakdown = alldata.groupby('topicCategory').size().reset_index(name='counts')\n",
    "testresults = []\n",
    "for eachtopic in breakdown['topicCategory'].tolist():\n",
    "    print(eachtopic)\n",
    "    training_set = generate_training_df(alldata,eachtopic)\n",
    "    X = vectorize_text(training_set)\n",
    "    for classifier in classifiers.keys():\n",
    "        timestart = datetime.now()\n",
    "        print(classifier)\n",
    "        cmresult,report,auc = train_test_classify(classifiers[classifier],training_set,X)\n",
    "        runtime = datetime.now() - timestart\n",
    "        testresults.append({'topicCategory':eachtopic,'set size':len(training_set),'classifier':classifier,\n",
    "                            'runtime':runtime,'auc':auc,'report':report,'matrix':cmresult})\n",
    "\n",
    "testresultsdf = pd.DataFrame(testresults)\n",
    "#testresultsdf.to_csv(os.path.join(RESULTPATH,'classifier_test.tsv'),sep='\\t',header=True)\n",
    "#print(testresultsdf)\n",
    "\n",
    "#max_auc = testresultsdf.groupby(['topicCategory','set size']).auc.max()\n",
    "max_auc = testresultsdf.groupby('classifier')['auc'].max()\n",
    "min_time = testresultsdf.groupby('classifier')['runtime'].min()\n",
    "avg_auc = testresultsdf.groupby('classifier')['auc'].mean()\n",
    "avg_time = testresultsdf.groupby('classifier')['runtime'].sum()\n",
    "sorted_results = testresultsdf.sort_values('auc',ascending=False)\n",
    "print(sorted_results)\n",
    "#print(sorted_results.iloc[0]['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.to_csv(os.path.join(RESULTPATH,'unambiguous_categories_data.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata=read_csv(os.path.join(RESULTPATH,'unambiguous_categories_data.tsv'),delimiter='\\t',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression and multinomialnb classifiers appear to be giving the highest AUC while having the shortest run times. Random forest provides decent results but has much longer run times. Using different classifiers and identifying the ones they agree on, could be a way to improve confidence when classify preprints into general litcovid categories. Using the disagreement between the classifiers may be able help to identify the ambiguous instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In depth testing\n",
    "Run sampling and testing 5 times, calculate average auc, create average report to evaluate performance of each of the three algorithms on the different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alldata = fetch_categorized_data(unambiguous)\n",
    "breakdown = alldata.groupby('topicCategory').size().reset_index(name='counts')\n",
    "testresults = []\n",
    "for eachtopic in breakdown['topicCategory'].tolist():\n",
    "    training_set = generate_training_df(alldata,eachtopic)\n",
    "    X = vectorize_text(training_set)\n",
    "    for classifier in classifiers.keys():\n",
    "        i=0\n",
    "        while i<5:\n",
    "            timestart = datetime.now()\n",
    "            cmresult,report,auc = train_test_classify(classifiers[classifier],training_set,X,i)\n",
    "            runtime = datetime.now() - timestart\n",
    "            testresults.append({'topicCategory':eachtopic,'set size':len(training_set),'classifier':classifier,\n",
    "                                'runtime':runtime,'auc':auc,'report':report,'matrix':cmresult,'i':i})\n",
    "            i=i+1\n",
    "\n",
    "testresultsdf = pd.DataFrame(testresults)\n",
    "testresultsdf.to_csv(os.path.join(RESULTPATH,'in_depth_classifier_test.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Testing: Ambiguous litcovid categories for classification verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguousdf = topicsdf.loc[topicsdf['_id'].isin(ambiguous)]\n",
    "amdata = fetch_categorized_data(ambiguousdf)\n",
    "ambreakdown = amdata.groupby('topicCategory').size().reset_index(name='counts')\n",
    "#amdata.to_csv(os.path.join(RESULTPATH,'ambiguous_categories_data.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved models\n",
    "filename = os.path.join(MODELPATH,'MultinomialNB_Treatment.sav')\n",
    "classifier = pickle.load(open(filename, 'rb'))\n",
    "vectorizername = os.path.join(MODELPATH,'vectorizer_Treatment.pickle')\n",
    "vectorizer = pickle.load(open(vectorizername,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treatment = amdata.loc[amdata['topicCategory']=='Treatment']\n",
    "labels = amdata['_id']\n",
    "M = vectorizer.transform(amdata['text'])\n",
    "prediction = classifier.predict(M)\n",
    "list_of_tuples = list(zip(labels,prediction))\n",
    "predictiondf = pd.DataFrame(list_of_tuples, columns = ['_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkdf = predictiondf.merge(treatment,on='_id',how='left')\n",
    "trueneg = checkdf.loc[((checkdf['prediction']=='not in category')&(checkdf['topicCategory']!='Treatment'))]\n",
    "truepos = checkdf.loc[((checkdf['prediction']=='in category')&(checkdf['topicCategory']=='Treatment'))]\n",
    "falseneg = checkdf.loc[((checkdf['prediction']=='not in category')&(checkdf['topicCategory']=='Treatment'))]\n",
    "falsepos = checkdf.loc[((checkdf['prediction']=='in category')&(checkdf['topicCategory']!='Treatment'))]\n",
    "print(\"total predictions: \",len(checkdf))\n",
    "print(\"true negative: \",len(trueneg),\" or \",len(trueneg)/len(checkdf)*100, '%')\n",
    "print(\"true positive: \",len(truepos),\" or \",len(truepos)/len(checkdf)*100, '%')\n",
    "print(\"false negative: \",len(falseneg),\" or \",len(falseneg)/len(checkdf)*100, '%')\n",
    "print(\"false positive: \",len(falsepos),\" or \",len(falsepos)/len(checkdf)*100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand beyond training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the vectorizer and preferred classifiers on the litcovid dataset with unambiguous categories and save the models\n",
    "#alldata = fetch_categorized_data(unambiguous)\n",
    "alldata = fetch_categorized_data(topicsdf)\n",
    "breakdown = alldata.groupby('topicCategory').size().reset_index(name='counts')\n",
    "\n",
    "for eachtopic in breakdown['topicCategory'].tolist():\n",
    "    trainingset = generate_training_df(alldata,eachtopic)\n",
    "    X = generate_vectorizer(trainingset,eachtopic)\n",
    "    for eachclassifier in classifiers.keys():\n",
    "        classifier=classifiers[eachclassifier]\n",
    "        classifier.fit(X, trainingset.target)\n",
    "        save_model(classifier,classname,category)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprints for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the preprint dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fetching preprint ids: ',datetime.now())\n",
    "preprint_ids = get_preprint_ids()\n",
    "print('fetching preprint abstracts: ',datetime.now())\n",
    "preprintdf = batch_fetch_meta(preprint_ids)\n",
    "print('cleaning up text: ',datetime.now())\n",
    "preprintdata = merge_texts(preprintdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply classifiers to predict classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topiclist = breakdown['topicCategory'].tolist()\n",
    "classifierlist = ['Logistic Regression','MultinomialNB','Random Forest']\n",
    "predict_class(topiclist,classifierlist,preprintdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate classifier predictions and identify areas of agreement and disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_agree = filter_agreement(eachtopic,classifierlist,agreetype='perfect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean up the results in case of multicategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanresults = clean_results(perfect_agree)\n",
    "print(len(cleanresults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = preprintdata['_id']\n",
    "for eachtopic in breakdown['topicCategory'].tolist():\n",
    "    classifierlist = ['Logistic Regression','MultinomialNB','Random Forest']\n",
    "    vectorizer = load_vectorizer(eachtopic)\n",
    "    M = vectorizer.transform(preprintdata['text'])\n",
    "    for eachclassifier in classifierlist:\n",
    "        classifierfile = os.path.join(MODELPATH, eachclassifier+\"_\"+eachtopic+'.sav')\n",
    "        classifier = pickle.load(open(classifierfile, 'rb'))\n",
    "        prediction = classifier.predict(M)\n",
    "        list_of_tuples = list(zip(labels,prediction))\n",
    "        predictiondf = pd.DataFrame(list_of_tuples, columns = ['_id', 'prediction'])\n",
    "        predictiondf['topicCategory']=eachtopic\n",
    "        predictiondf['classifier']=eachclassifier\n",
    "        predictiondf.to_csv(os.path.join(RESULTPATH,'predictions/'+eachtopic+\"_\"+eachclassifier+'.tsv'),sep='\\t',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Vectorize the text for classifier\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(training_set['text'])\n",
    "features = vectorizer.get_feature_names()\n",
    "print(X.shape)\n",
    "\n",
    "#### Split the data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, training_set.target, test_size=0.2, random_state=0)\n",
    "\n",
    "#### Classify training text as in category or not in category\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "report = classification_report(y_test,y_pred,output_dict=True)\n",
    "print(pd.DataFrame(report))\n",
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(auc)\n",
    "print('[[true neg     false pos]]')\n",
    "print('[[false neg     true pos]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_results = classifier_results.groupby('treatment_prediction').size().reset_index(name='counts')\n",
    "print(inspect_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
