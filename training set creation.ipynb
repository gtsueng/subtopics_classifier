{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training datasets\n",
    "The process for creating the initial training datasets for the broad topicCategories NOT included in litcovid are as follows:\n",
    "1. Brainstorm keywords that should be relatively SPECIFIC for that topicCategory\n",
    "2. Perform queries to the APIs to retrieve all ids returned by each topicCategory-specific keyword\n",
    "3. Retrieve all keywords for each id\n",
    "4. Create frequency tables of all keywords\n",
    "5. Take top ~5% of most frequent keywords\n",
    "6. Take top 100 most frequent keywords from litcovid topics\n",
    "8. Use the keywords to remove more generic terms like 'COVID19'\n",
    "9. manually inspect for specific-enough terms for inclusion into original keyword list\n",
    "10. Repeat process, adding keywords to grow the training dataset, or removing keywords which are causing the dataset to become less specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset based on initial curated keyword lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "#import pickle\n",
    "#import sklearn\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/'\n",
    "KEYPATH = os.path.join(DATAPATH,'keywords/')\n",
    "KEYFILES = os.listdir(KEYPATH)\n",
    "RESULTSPATH = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'https://api.outbreak.info/resources/resource/query?q=loneliness&filter=@type:Publication&fields=_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for using the outbreak api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "def fetch_query_size(query):\n",
    "    pubmeta = requests.get('https://api.outbreak.info/resources/query?q=((\"'+query+'\") AND (@type:Publication))&size=0&aggs=@type')\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_query_ids(query):\n",
    "    query_size = fetch_query_size(query)\n",
    "    r = requests.get('https://api.outbreak.info/resources/query?q=((\"'+query+'\") AND (@type:Publication))&fields=_id&fetch_all=true')\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < query_size:\n",
    "            r2 = requests.get('https://api.outbreak.info/resources/query?q=((\"'+query+'\") AND (@type:Publication))&fields=_id&fetch_all=true&scroll_id='+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "\n",
    "def batch_fetch_kw(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    keywordsdf = pd.DataFrame(columns = ['_id','keywords'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'keywords'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pd.read_json(r.text)\n",
    "            cleanresult = rawresult[['_id','keywords']].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            keywordsdf = pd.concat((keywordsdf,cleanresult))\n",
    "        i=i+1\n",
    "    return(keywordsdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to load the initial search terms and litcovid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_search_terms():\n",
    "    keyword_dict = {}\n",
    "    for eachfile in KEYFILES:\n",
    "        filename = eachfile.split('.')[0]\n",
    "        keywords = []\n",
    "        with open(os.path.join(KEYPATH,eachfile),'r') as readfile:\n",
    "            for eachline in readfile:\n",
    "                keywords.append(eachline.strip())\n",
    "        keyword_dict[filename]=keywords\n",
    "    return(keyword_dict)\n",
    "\n",
    "\n",
    "def load_category_ids():\n",
    "    keyword_dict = load_search_terms()\n",
    "    cat_dict = {}\n",
    "    for category in keyword_dict.keys():\n",
    "        allids = []\n",
    "        idlist = []\n",
    "        keywordlist = keyword_dict[category]\n",
    "        for eachkey in keywordlist:\n",
    "            idlist = get_query_ids(eachkey)\n",
    "            allids = list(set(allids).union(set(idlist)))\n",
    "        cat_dict[category]=allids\n",
    "    return(cat_dict)\n",
    "\n",
    "\n",
    "def load_litcovid_ids():\n",
    "    litcovid = read_csv(os.path.join(DATAPATH,'litcovidtopics.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "    topics = litcovid['topicCategory'].unique().tolist()\n",
    "    litcovid_dict={}\n",
    "    for eachtopic in topics:\n",
    "        tmplist = litcovid['_id'].loc[litcovid['topicCategory']==eachtopic].tolist()\n",
    "        litcovid_dict[eachtopic]=tmplist\n",
    "    return(litcovid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for creating keyword frequency tables for evaluation (and manual inspection)\n",
    "\n",
    "Expand the search based on keywords that appeared from the first round of searching. Remove generic terms (such as COVID-19) by generating the top most frequent keywords from LitCovid. \n",
    "\n",
    "1. For the first pass, create a frequency table, keeping only the top 5% non-generic keywords for evaluation\n",
    "2. For the second pass (with an expanded search term list), keep only the top 1% of terms for evaluation as 5% may be too large a set to evaluate\n",
    "3. After reviewing the 1% from the second pass, a 3rd pass was determined to be unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Obtain the most frequent (ie- generic) keywordss from litcovidtopics\n",
    "#### Default is top 100 most generic, but this can be adjusted\n",
    "def get_generic_terms(top=100):\n",
    "    litcovid = read_csv(os.path.join(DATAPATH,'litcovidtopics.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "    idlist = litcovid['_id'].unique().tolist()\n",
    "    tmpdf = batch_fetch_kw(idlist)\n",
    "    kwdf = tmpdf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "    kwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "    kwdf['keywords']=kwdf['keywords'].astype(str).str.lower()\n",
    "    kwfreq = kwdf.groupby('keywords').size().reset_index(name='count')\n",
    "    kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "    return(kwfreq.head(n=top).copy())\n",
    "\n",
    "\n",
    "def get_kw_freq(topicCategory):\n",
    "    cat_dict = load_category_ids()\n",
    "    litcovid_dict = load_litcovid_ids()\n",
    "    if topicCategory in cat_dict.keys():\n",
    "        idlist = cat_dict[topicCategory]\n",
    "    else:\n",
    "        idlist = litcovid_dict[topicCategory]\n",
    "    tmpdf = batch_fetch_kw(idlist)\n",
    "    kwdf = tmpdf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "    kwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "    kwdf['keywords']=kwdf['keywords'].astype(str).str.lower()\n",
    "    kwfreq = kwdf.groupby('keywords').size().reset_index(name='count')\n",
    "    kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "    return(kwfreq)\n",
    "\n",
    "\n",
    "#### Set the count of the generic keywords to -1, and re-sort the list\n",
    "def negate_genkw(kwfreq,genkw):\n",
    "    genkwlist = genkw['keywords'].tolist()\n",
    "    kwfreq['count'].loc[kwfreq['keywords'].isin(genkwlist)]=-1\n",
    "    kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "    return(kwfreq)\n",
    "\n",
    "\n",
    "#### Export the most frequent x% of keywords\n",
    "#### Note that 5(%) is default, but any number can be used\n",
    "#### Also, the topicCategory is required for filename purposes\n",
    "def export_most_freqkw(kwfreq,topicCategory,toppercent=5):\n",
    "    export_num = int(round(len(kwfreq)*toppercent/100,0))\n",
    "    exportdf = kwfreq.head(n=export_num)\n",
    "    filepath = RESULTSPATH+'wordfrequencies/'\n",
    "    filename = topicCategory+'_'+str(toppercent)+'_percent_most_freq_kw.txt'\n",
    "    exportdf.to_csv(os.path.join(filepath,filename),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for comparing search results from Outbreak and LitCovid APIs to generate the training dataset\n",
    "\n",
    "Also, hit the LitCovid API to generate initial list of pmids. Then select the pmids that were returned by BOTH the Outbreak API and the LitCovid API to get the ones that are more likely to be on-topic.\n",
    "\n",
    "Eg- 'https://www.ncbi.nlm.nih.gov/research/coronavirus-api/export/tsv?text=%22emission%20reduction%22&filters=%7B%7D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search litcovid for a term and retrieve the pmids\n",
    "def search_litcovid_ids(searchterm):\n",
    "    check_litcovid = requests.get('https://www.ncbi.nlm.nih.gov/research/coronavirus-api/export/tsv?text=\"'+searchterm+'\"&filters={}')\n",
    "    litcovid_data = check_litcovid.text.split('\\n')[34:]\n",
    "    pmids = []\n",
    "    for line in litcovid_data:\n",
    "        if line.startswith('#') or line.startswith('p'):\n",
    "            continue\n",
    "        pmids.append(line.split('\\t')[0])\n",
    "    cleanpmids = ['pmid'+x for x in pmids if x != \"\"]\n",
    "    return(cleanpmids)\n",
    "\n",
    "## load the list of search terms for each topicCategory and push it to either outbreak (default) or litcovid\n",
    "## returns an id list\n",
    "def category_id_check(source='outbreak'):\n",
    "    keyword_dict = load_search_terms()\n",
    "    allids = []\n",
    "    if source == 'litcovid':\n",
    "        for category in keyword_dict.keys():  \n",
    "            keywordlist = keyword_dict[category]\n",
    "            for eachkey in keywordlist:\n",
    "                idlist = search_litcovid_ids(eachkey)\n",
    "                allids.append({'category':category,'searchterm':eachkey,'ids':idlist})\n",
    "        idcheck = pd.DataFrame(allids)\n",
    "    else:\n",
    "        for category in keyword_dict.keys():  \n",
    "            keywordlist = keyword_dict[category]\n",
    "            for eachkey in keywordlist:\n",
    "                idlist = get_query_ids(eachkey)\n",
    "                allids.append({'category':category,'searchterm':eachkey,'ids':idlist})\n",
    "        idcheck = pd.DataFrame(allids)\n",
    "    return(idcheck)\n",
    "\n",
    "\n",
    "## Pull the id lists after search outbreak and litcovid and compare them\n",
    "## keep only ids in common for training purposes\n",
    "## Note, this will remove all preprints from the training set since litcovid does not have them\n",
    "def get_in_common_ids():\n",
    "    outbreakids = category_id_check()\n",
    "    litcovidids = category_id_check(source='litcovid')\n",
    "    mergedf = outbreakids.merge(litcovidids,on=(['category','searchterm']),how='outer')\n",
    "    i=0\n",
    "    tmplist = []\n",
    "    while i <len(mergedf):\n",
    "        idsincommon = list(set(mergedf.iloc[i]['ids_x']).intersection(set(mergedf.iloc[i]['ids_y'])))\n",
    "        tmplist.append({'category':mergedf.iloc[i]['category'],\n",
    "                        'searchterm':mergedf.iloc[i]['searchterm'],\n",
    "                        'len_ids_x':len(mergedf.iloc[i]['ids_x']),\n",
    "                        'len_ids_y':len(mergedf.iloc[i]['ids_y']),\n",
    "                        'len_clean_ids':len(idsincommon),\n",
    "                        'clean_ids':idsincommon})\n",
    "        i=i+1\n",
    "    cleandf = pd.DataFrame(tmplist)\n",
    "    return(cleandf)   \n",
    "\n",
    "\n",
    "def generate_training_dict(savefile = False):\n",
    "    cleandf = get_in_common_ids()\n",
    "    training_dict = {}\n",
    "    for eachcat in cleandf['category'].unique().tolist(): \n",
    "        j=0\n",
    "        tmpdf = cleandf.loc[cleandf['category']==eachcat]\n",
    "        allids = []\n",
    "        while j<len(tmpdf):\n",
    "            allids = list(set(allids).union(set(tmpdf.iloc[j]['clean_ids'])))\n",
    "            j=j+1\n",
    "        training_dict[eachcat]=allids\n",
    "    if savefile == False:\n",
    "        return(training_dict)\n",
    "    else:\n",
    "        with open(os.path.join(RESULTSPATH,\"training_dict.json\"), \"w\") as outfile: \n",
    "            json.dump(training_dict, outfile)        \n",
    "\n",
    "def transform_training_dict(training_dict):\n",
    "    trainingdf = pd.DataFrame(columns=['_id','topicCategory'])\n",
    "    for eachcat in training_dict.keys():\n",
    "        idlist = pd.DataFrame(training_dict[eachcat])\n",
    "        idlist.rename(columns={0:'_id'},inplace=True)\n",
    "        idlist['topicCategory']=eachcat\n",
    "        trainingdf = pd.concat((trainingdf,idlist),ignore_index=True)\n",
    "    return(trainingdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main script for generating training dataset (assuming manual expansion of search terms was performed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict = generate_training_dict()\n",
    "trainingdf = transform_training_dict(training_dict)\n",
    "print(trainingdf['topicCategory'].unique().tolist(),len(trainingdf))\n",
    "trainingdf.to_csv(os.path.join(RESULTSPATH,'training_ids.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export training set as topics to be added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(allresults):\n",
    "    counts = allresults.groupby('_id').size().reset_index(name='counts')\n",
    "    duplicates = counts.loc[counts['counts']>1]\n",
    "    singles = counts.loc[counts['counts']==1]\n",
    "    dupids = duplicates['_id'].unique().tolist()\n",
    "    tmplist = []\n",
    "    for eachid in dupids:\n",
    "        catlist = allresults['topicCategory'].loc[allresults['_id']==eachid].tolist()\n",
    "        tmplist.append({'_id':eachid,'topicCategory':catlist})\n",
    "    tmpdf = pd.DataFrame(tmplist)  \n",
    "    tmpsingledf = allresults[['_id','topicCategory']].loc[allresults['_id'].isin(singles['_id'].tolist())]\n",
    "    idlist = tmpsingledf['_id'].tolist()\n",
    "    catlist = tmpsingledf['topicCategory'].tolist()\n",
    "    cattycat = [[x] for x in catlist]\n",
    "    list_of_tuples = list(zip(idlist,cattycat))\n",
    "    singledf = pd.DataFrame(list_of_tuples, columns = ['_id', 'topicCategory']) \n",
    "    cleanresults = pd.concat((tmpdf,singledf),ignore_index=True)\n",
    "    return(cleanresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict = generate_training_dict()\n",
    "trainingdf = transform_training_dict(training_dict)\n",
    "cleanresults = clean_results(trainingdf)\n",
    "cleanresults.to_json(os.path.join(RESULTSPATH,'topicCats.json'),orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual inspection\n",
    "This section contains script that was run for the purposes of manually inspecting the data in order to determine suitable default cut off points and threshholds.\n",
    "\n",
    "### How to determine if a keyword is suitable for inclusion on a topicCategory-specific list\n",
    "1. Verify that a more generic version of the word is not already available: ie- if the keyword is 'manic depression', but the term 'depression' is already on the list, it should not be necessary to add 'manic depression'\n",
    "2. If a more generic version of the word is not already available, search the term on outbreak.info/resources to see if appropriate results pop up.\n",
    "3. If criteria 2 is met, perform a litcovid search for the term and scan the titles and abstracts. The top 10 most relevant results should be within topic, and 4/5 of the least relevant results should also be within topic.\n",
    "4. Also check that 12/20 of the most recent results are also within topic. This is to help account for the creation of off-topic new terms/phrases which may enclose the keyword, or changes in connotations based on trends.\n",
    "5. If criteria 1-4 are met, add the term to the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main script for manual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genkw = get_generic_terms()\n",
    "cat_dict = load_category_ids()\n",
    "for everytopic in cat_dict.keys():\n",
    "    kwfreq = get_kw_freq(everytopic)\n",
    "    kwfreq = negate_genkw(kwfreq,genkw)\n",
    "    export_most_freqkw(kwfreq,everytopic,toppercent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Expand on keywordlist pulling the keywords for each id, generate frequency list\n",
    "#### Verify that most common conceptually related keywords are in the list\n",
    "\n",
    "## fetch the keywords for every pmid in the environment list\n",
    "environmentdf = batch_fetch_kw(cat_dict['Environment'])\n",
    "\n",
    "## unstack the dataframe and lower case the keywords for consistency\n",
    "envkwdf = environmentdf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "envkwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "envkwdf['keywords']=envkwdf['keywords'].astype(str).str.lower()\n",
    "\n",
    "## Determine keyword frequency\n",
    "kwfreq = envkwdf.groupby('keywords').size().reset_index(name='count')\n",
    "kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "print(len(kwfreq))\n",
    "\n",
    "#### Inspect number of words within various frequency categories\n",
    "## Identify length of keywords short enough for review\n",
    "min20 = kwfreq.loc[kwfreq['count']>20]\n",
    "min10 = kwfreq.loc[kwfreq['count']>10]\n",
    "min5 = kwfreq.loc[kwfreq['count']>5]\n",
    "min3 = kwfreq.loc[kwfreq['count']>3]\n",
    "min2 = kwfreq.loc[kwfreq['count']>2]\n",
    "print(len(min20),len(min10),len(min5),len(min3),len(min2))\n",
    "\n",
    "#### export results and inspect for potential keywords that are specific enough to this topicCategory\n",
    "## Do not use words that may be frequently used outside of this topicCategory\n",
    "## Eg- Behavior: 'stress' is too generic, may refer to oxidative stress\n",
    "## Eg- Environment: 'climate' is too generic, may refer to current trends or the work environment\n",
    "## In this case, we're going with words that were mentioned by at least 3 pmids (ie- ~top 5% most frequent words)\n",
    "min3.to_csv(os.path.join(RESULTSPATH,'wordfrequencies/environment_min3.txt'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch the keywords for every pmid in the environment list\n",
    "behavedf = batch_fetch_kw(cat_dict['Behavioral Research'])\n",
    "\n",
    "## unstack the dataframe and lower case the keywords for consistency\n",
    "bhkwdf = behavedf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "bhkwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "bhkwdf['keywords']=bhkwdf['keywords'].astype(str).str.lower()\n",
    "\n",
    "## Determine keyword frequency\n",
    "kwfreq = bhkwdf.groupby('keywords').size().reset_index(name='count')\n",
    "kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "\n",
    "print(len(kwfreq))\n",
    "\n",
    "#### Inspect number of words within various frequency categories\n",
    "## Identify length of keywords short enough for review\n",
    "min20 = kwfreq.loc[kwfreq['count']>20]\n",
    "min10 = kwfreq.loc[kwfreq['count']>10]\n",
    "min7 = kwfreq.loc[kwfreq['count']>7]\n",
    "min5 = kwfreq.loc[kwfreq['count']>5]\n",
    "min3 = kwfreq.loc[kwfreq['count']>3]\n",
    "min2 = kwfreq.loc[kwfreq['count']>2]\n",
    "print(len(min20),len(min10),len(min7),len(min5),len(min3),len(min2))\n",
    "\n",
    "#### export results and inspect for potential keywords that are specific enough to this topicCategory\n",
    "## Do not use words that may be frequently used outside of this topicCategory\n",
    "## Eg- Behavior: 'stress' is too generic, may refer to oxidative stress\n",
    "## Eg- Environment: 'climate' is too generic, may refer to current trends or the work environment\n",
    "## In this case, we're going with words that were mentioned by at least 7 pmids (ie- ~top 5% most frequent words)\n",
    "min7.to_csv(os.path.join(RESULTSPATH,'wordfrequencies/behavior_min7.txt'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
