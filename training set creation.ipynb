{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training datasets\n",
    "The process for creating the initial training datasets for the broad topicCategories NOT included in litcovid are as follows:\n",
    "1. Brainstorm keywords that should be relatively SPECIFIC for that topicCategory\n",
    "2. Perform queries to the APIs to retrieve all ids returned by each topicCategory-specific keyword\n",
    "3. Retrieve all keywords for each id\n",
    "4. Create frequency tables of all keywords\n",
    "5. Take top ~5% of most frequent keywords\n",
    "6. Take top 100 most frequent keywords from litcovid topics\n",
    "8. Use the keywords to remove more generic terms like 'COVID19'\n",
    "9. manually inspect for specific-enough terms for inclusion into original keyword list\n",
    "10. Repeat process, adding keywords to grow the training dataset, or removing keywords which are causing the dataset to become less specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset based on initial curated keyword lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "#import pickle\n",
    "#import sklearn\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/'\n",
    "KEYPATH = os.path.join(DATAPATH,'keywords/')\n",
    "KEYFILES = os.listdir(KEYPATH)\n",
    "RESULTSPATH = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'https://api.outbreak.info/resources/resource/query?q=loneliness&filter=@type:Publication&fields=_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "def fetch_query_size(query):\n",
    "    pubmeta = requests.get('https://api.outbreak.info/resources/query?q=\"'+query+'\"&size=0&aggs=@type')\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_query_ids(query):\n",
    "    query_size = fetch_query_size(query)\n",
    "    r = requests.get('https://api.outbreak.info/resources/query?q=\"'+query+'\"&filter=@type:Publication&fields=_id&fetch_all=true')\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < query_size:\n",
    "            r2 = requests.get('https://api.outbreak.info/resources/query?q=\"'+query+'\"&filter=@type:Publication&fields=_id&fetch_all=true&scroll_id='+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "\n",
    "def batch_fetch_meta(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pd.DataFrame(columns = ['_id','abstract','name'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pd.read_json(r.text)\n",
    "            cleanresult = rawresult[['_id','name','abstract',]].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            textdf = pd.concat((textdf,cleanresult))\n",
    "        i=i+1\n",
    "    return(textdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_fetch_kw(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    keywordsdf = pd.DataFrame(columns = ['_id','keywords'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'keywords'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pd.read_json(r.text)\n",
    "            cleanresult = rawresult[['_id','keywords']].loc[rawresult['_score']==1].copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            keywordsdf = pd.concat((keywordsdf,cleanresult))\n",
    "        i=i+1\n",
    "    return(keywordsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_search_terms():\n",
    "    keyword_dict = {}\n",
    "    for eachfile in KEYFILES:\n",
    "        filename = eachfile.split('.')[0]\n",
    "        keywords = []\n",
    "        with open(os.path.join(KEYPATH,eachfile),'r') as readfile:\n",
    "            for eachline in readfile:\n",
    "                keywords.append(eachline.strip())\n",
    "        keyword_dict[filename]=keywords\n",
    "    return(keyword_dict)\n",
    "\n",
    "def load_category_ids():\n",
    "    keyword_dict = load_search_terms()\n",
    "    cat_dict = {}\n",
    "    for category in keyword_dict.keys():\n",
    "        allids = []\n",
    "        keywordlist = keyword_dict[category]\n",
    "        for eachkey in keywordlist:\n",
    "            idlist = get_query_ids(eachkey)\n",
    "            allids = list(set(allids).union(set(idlist)))\n",
    "        cat_dict[category]=allids\n",
    "    return(cat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_litcovid_ids():\n",
    "    litcovid = read_csv(os.path.join(DATAPATH,'litcovidtopics.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "    topics = litcovid['topicCategory'].unique().tolist()\n",
    "    litcovid_dict={}\n",
    "    for eachtopic in topics:\n",
    "        tmplist = litcovid['_id'].loc[litcovid['topicCategory']==eachtopic].tolist()\n",
    "        litcovid_dict[eachtopic]=tmplist\n",
    "    return(litcovid_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kw_freq(topicCategory):\n",
    "    cat_dict = load_category_ids()\n",
    "    litcovid_dict = load_litcovid_ids()\n",
    "    if topicCategory in cat_dict.keys():\n",
    "        idlist = cat_dict[topicCategory]\n",
    "    else:\n",
    "        idlist = litcovid_dict[topicCategory]\n",
    "    tmpdf = batch_fetch_kw(idlist)\n",
    "    kwdf = tmpdf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "    kwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "    kwdf['keywords']=kwdf['keywords'].astype(str).str.lower()\n",
    "    kwfreq = kwdf.groupby('keywords').size().reset_index(name='count')\n",
    "    kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "    return(kwfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Obtain the most frequent (ie- generic) keywordss from litcovidtopics\n",
    "#### Default is top 100 most generic, but this can be adjusted\n",
    "def get_generic_terms(top=100):\n",
    "    litcovid = read_csv(os.path.join(DATAPATH,'litcovidtopics.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "    idlist = litcovid['_id'].unique().tolist()\n",
    "    tmpdf = batch_fetch_kw(idlist)\n",
    "    kwdf = tmpdf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "    kwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "    kwdf['keywords']=kwdf['keywords'].astype(str).str.lower()\n",
    "    kwfreq = kwdf.groupby('keywords').size().reset_index(name='count')\n",
    "    kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "    return(kwfreq.head(n=top).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set the count of the generic keywords to -1, and re-sort the list\n",
    "\n",
    "def negate_genkw(kwfreq,genkw):\n",
    "    genkwlist = genkw['keywords'].tolist()\n",
    "    kwfreq['count'].loc[kwfreq['keywords'].isin(genkwlist)]=-1\n",
    "    kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "    return(kwfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Export the most frequent x% of keywords\n",
    "#### Note that 5(%) is default, but any number can be used\n",
    "#### Also, the topicCategory is required for filename purposes\n",
    "\n",
    "def export_most_freqkw(kwfreq,topicCategory,toppercent=5):\n",
    "    export_num = int(round(len(kwfreq)*toppercent/100,0))\n",
    "    exportdf = kwfreq.head(n=export_num)\n",
    "    filepath = RESULTSPATH+'wordfrequencies/'\n",
    "    filename = topicCategory+'_'+str(toppercent)+'_percent_most_freq_kw.txt'\n",
    "    exportdf.to_csv(os.path.join(filepath,filename),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16927\n",
      "2018\n"
     ]
    }
   ],
   "source": [
    "print(len(cat_dict['Behavioral Research']))\n",
    "print(len(cat_dict['Environment']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main Script\n",
    "genkw = get_generic_terms()\n",
    "cat_dict = load_category_ids()\n",
    "for everytopic in cat_dict.keys():\n",
    "    kwfreq = get_kw_freq(everytopic)\n",
    "    kwfreq = negate_genkw(kwfreq,genkw)\n",
    "    export_most_freqkw(kwfreq,everytopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual inspection\n",
    "This section contains script that was run for the purposes of manually inspecting the data in order to determine suitable default cut off points and threshholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3638\n"
     ]
    }
   ],
   "source": [
    "#### To do, expand on keywordlist pulling the keywords for each id, generate frequency list\n",
    "#### Verify that most common conceptually related keywords are in the list\n",
    "\n",
    "## fetch the keywords for every pmid in the environment list\n",
    "environmentdf = batch_fetch_kw(cat_dict['Environment'])\n",
    "\n",
    "## unstack the dataframe and lower case the keywords for consistency\n",
    "envkwdf = environmentdf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "envkwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "envkwdf['keywords']=envkwdf['keywords'].astype(str).str.lower()\n",
    "\n",
    "## Determine keyword frequency\n",
    "kwfreq = envkwdf.groupby('keywords').size().reset_index(name='count')\n",
    "kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "print(len(kwfreq))\n",
    "\n",
    "#### Inspect number of words within various frequency categories\n",
    "## Identify length of keywords short enough for review\n",
    "min20 = kwfreq.loc[kwfreq['count']>20]\n",
    "min10 = kwfreq.loc[kwfreq['count']>10]\n",
    "min5 = kwfreq.loc[kwfreq['count']>5]\n",
    "min3 = kwfreq.loc[kwfreq['count']>3]\n",
    "min2 = kwfreq.loc[kwfreq['count']>2]\n",
    "print(len(min20),len(min10),len(min5),len(min3),len(min2))\n",
    "\n",
    "#### export results and inspect for potential keywords that are specific enough to this topicCategory\n",
    "## Do not use words that may be frequently used outside of this topicCategory\n",
    "## Eg- Behavior: 'stress' is too generic, may refer to oxidative stress\n",
    "## Eg- Environment: 'climate' is too generic, may refer to current trends or the work environment\n",
    "## In this case, we're going with words that were mentioned by at least 3 pmids (ie- ~top 5% most frequent words)\n",
    "min3.to_csv(os.path.join(RESULTSPATH,'wordfrequencies/environment_min3.txt'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch the keywords for every pmid in the environment list\n",
    "behavedf = batch_fetch_kw(cat_dict['Behavioral Research'])\n",
    "\n",
    "## unstack the dataframe and lower case the keywords for consistency\n",
    "bhkwdf = behavedf.set_index('_id').keywords.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(str).reset_index()\n",
    "bhkwdf.rename(columns={0:'keywords'},inplace=True)\n",
    "bhkwdf['keywords']=bhkwdf['keywords'].astype(str).str.lower()\n",
    "\n",
    "## Determine keyword frequency\n",
    "kwfreq = bhkwdf.groupby('keywords').size().reset_index(name='count')\n",
    "kwfreq.sort_values('count',ascending=False,inplace=True)\n",
    "\n",
    "print(len(kwfreq))\n",
    "\n",
    "#### Inspect number of words within various frequency categories\n",
    "## Identify length of keywords short enough for review\n",
    "min20 = kwfreq.loc[kwfreq['count']>20]\n",
    "min10 = kwfreq.loc[kwfreq['count']>10]\n",
    "min7 = kwfreq.loc[kwfreq['count']>7]\n",
    "min5 = kwfreq.loc[kwfreq['count']>5]\n",
    "min3 = kwfreq.loc[kwfreq['count']>3]\n",
    "min2 = kwfreq.loc[kwfreq['count']>2]\n",
    "print(len(min20),len(min10),len(min7),len(min5),len(min3),len(min2))\n",
    "\n",
    "#### export results and inspect for potential keywords that are specific enough to this topicCategory\n",
    "## Do not use words that may be frequently used outside of this topicCategory\n",
    "## Eg- Behavior: 'stress' is too generic, may refer to oxidative stress\n",
    "## Eg- Environment: 'climate' is too generic, may refer to current trends or the work environment\n",
    "## In this case, we're going with words that were mentioned by at least 7 pmids (ie- ~top 5% most frequent words)\n",
    "min7.to_csv(os.path.join(RESULTSPATH,'wordfrequencies/behavior_min7.txt'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
