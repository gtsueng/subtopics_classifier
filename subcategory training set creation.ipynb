{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training datasets for specific topicCategories from LitCovid data\n",
    "\n",
    "This was the process used in creating an initial training set of pmids from LitCovid.\n",
    "1. Pull all LitCovid pmids with keywords field\n",
    "2. Sort keywords field by frequency\n",
    "3. Map keywords fields to topicCategories (as specifically as possible)\n",
    "4. Use mapping to identify pmids for each topicCategory (matching_pmids)\n",
    "5. For cleaner training data, remove ambiguous entries by filtering out duplicate pmids (filtered_pmids)\n",
    "\n",
    "To update the training dataset, repeat from step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import requests\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for fetching metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the size of the source (to make it easy to figure out when to stop scrolling)\n",
    "def fetch_src_size(source):\n",
    "    pubmeta = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&size=0&aggs=@type\")\n",
    "    pubjson = json.loads(pubmeta.text)\n",
    "    pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "    return(pubcount)\n",
    "\n",
    "#### Pull ids from a json file\n",
    "def get_ids_from_json(jsonfile):\n",
    "    idlist = []\n",
    "    for eachhit in jsonfile[\"hits\"]:\n",
    "        if eachhit[\"_id\"] not in idlist:\n",
    "            idlist.append(eachhit[\"_id\"])\n",
    "    return(idlist)\n",
    "\n",
    "#### Ping the API and get all the ids for a specific source and scroll through the source until number of ids matches meta\n",
    "def get_source_ids(source):\n",
    "    source_size = fetch_src_size(source)\n",
    "    r = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true\")\n",
    "    response = json.loads(r.text)\n",
    "    idlist = get_ids_from_json(response)\n",
    "    try:\n",
    "        scroll_id = response[\"_scroll_id\"]\n",
    "        while len(idlist) < source_size:\n",
    "            r2 = requests.get(\"https://api.outbreak.info/resources/query?q=((@type:Publication) AND (curatedBy.name:\"+source+\"))&fields=_id&fetch_all=true&scroll_id=\"+scroll_id)\n",
    "            response2 = json.loads(r2.text)\n",
    "            idlist2 = set(get_ids_from_json(response2))\n",
    "            tmpset = set(idlist)\n",
    "            idlist = tmpset.union(idlist2)\n",
    "            try:\n",
    "                scroll_id = response2[\"_scroll_id\"]\n",
    "            except:\n",
    "                print(\"no new scroll id\")\n",
    "        return(idlist)\n",
    "    except:\n",
    "        return(idlist)\n",
    "\n",
    "\n",
    "def get_pub_ids(sourceset):\n",
    "    pub_srcs = {\"preprint\":[\"bioRxiv\",\"medRxiv\"],\"litcovid\":[\"litcovid\"],\n",
    "                \"other\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\"],\n",
    "                \"all\":[\"Figshare\",\"Zenodo\",\"MRC Centre for Global Infectious Disease Analysis\",\n",
    "                       \"bioRxiv\",\"medRxiv\",\"litcovid\"]}\n",
    "    sourcelist = pub_srcs[sourceset]\n",
    "    allids = []\n",
    "    for eachsource in sourcelist:\n",
    "        sourceids = get_source_ids(eachsource)\n",
    "        allids = list(set(allids).union(set(sourceids)))\n",
    "    return(allids)\n",
    "\n",
    "\n",
    "#### Get the name, abstract for the pmids\n",
    "#### Note, I've tried batches of 1000, and the post request has failed, so this uses a batch size that's less likely to fail\n",
    "def batch_fetch_keywords(idlist):\n",
    "    ## Break the list of ids into smaller chunks so the API doesn't fail the post request\n",
    "    runs = round((len(idlist))/100,0)\n",
    "    i=0 \n",
    "    separator = ','\n",
    "    ## Create dummy dataframe to store the meta data\n",
    "    textdf = pd.DataFrame(columns = ['_id','abstract','name','keywords'])\n",
    "    while i < runs+1:\n",
    "        if len(idlist)<100:\n",
    "            sample = idlist\n",
    "        elif i == 0:\n",
    "            sample = idlist[i:(i+1)*100]\n",
    "        elif i == runs:\n",
    "            sample = idlist[i*100:len(idlist)]\n",
    "        else:\n",
    "            sample = idlist[i*100:(i+1)*100]\n",
    "        sample_ids = separator.join(sample)\n",
    "        ## Get the text-based metadata (abstract, title) and save it\n",
    "        r = requests.post(\"https://api.outbreak.info/resources/query/\", params = {'q': sample_ids, 'scopes': '_id', 'fields': 'name,abstract,keywords'})\n",
    "        if r.status_code == 200:\n",
    "            rawresult = pd.read_json(r.text)\n",
    "            checkcols = rawresult.columns\n",
    "            if (('keywords' not in checkcols) and ('abstract' in checkcols)):\n",
    "                rawresult['keywords']=[]\n",
    "            elif (('keywords' in checkcols) and ('abstract' not in checkcols)):\n",
    "                rawresult['abstract']=\" \"\n",
    "            elif (('keywords' not in checkcols) and ('abstract' not in checkcols)):\n",
    "                rawresult['abstract']=\" \"\n",
    "                rawresult['keywords']=[]\n",
    "            cleanresult = rawresult[['_id','name','abstract','keywords']].loc[rawresult['_score']==1].fillna(\" \").copy()\n",
    "            cleanresult.drop_duplicates(subset='_id',keep=\"first\", inplace=True)\n",
    "            textdf = pd.concat((textdf,cleanresult))\n",
    "        i=i+1\n",
    "    return(textdf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'\n",
    "MODELPATH = 'models/subtopics/'\n",
    "PREDICTPATH = 'predictions/subtopics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "litcovid_ids = get_pub_ids(\"litcovid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68895\n",
      "            _id                                           abstract  \\\n",
      "1  pmid33060058  OBJECTIVE: Encephalopathy is a major neurologi...   \n",
      "2  pmid32624494  Since the outbreak of COVID-19 in China in Dec...   \n",
      "3  pmid33192858  Objective: We aim to determine the psychologic...   \n",
      "4  pmid33583756  OBJECTIVE: To observe the early interventions ...   \n",
      "\n",
      "                                                name  \\\n",
      "1  Alpha coma EEG pattern in patients with severe...   \n",
      "2  Improved survival following ward-based non-inv...   \n",
      "3  Psychological Impact of the Civil War and COVI...   \n",
      "4  Early therapeutic interventions of traditional...   \n",
      "\n",
      "                                            keywords  \n",
      "1  [Ascending reticular formation, Brainstem, Enc...  \n",
      "2  [assisted ventilation, lung physiology, non in...  \n",
      "3  [COVID-19, GAD-7, PHQ-9, SARS-CoV-2, anxiety, ...  \n",
      "4  [Conversion time of viral nucleic acid, Corona...  \n",
      "Wall time: 19min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "textdf = batch_fetch_keywords(litcovid_ids)\n",
    "\n",
    "keywordsdf = textdf.loc[textdf['keywords'].str.len()>1].copy()\n",
    "print(len(keywordsdf))\n",
    "print(keywordsdf.head(n=4))\n",
    "\n",
    "with open(os.path.join(DATAPATH,'keywordsdf.pickle'),'wb') as keydumpfile:\n",
    "    pickle.dump(keywordsdf,keydumpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56006\n",
      "            _id                                           abstract  \\\n",
      "0  pmid32529889  Since the emergence of patients with COVID-19 ...   \n",
      "1  pmid33982129  This correspondence argues that it is not only...   \n",
      "2  pmid32779341  Severe acute respiratory syndrome coronavirus ...   \n",
      "5  pmid33645278  INTRODUCTION: The COVID-19 pandemic resulted i...   \n",
      "\n",
      "                                                name  \\\n",
      "0  Prevention and control strategies in the diagn...   \n",
      "1              The last rites in a time of pandemic.   \n",
      "2  Understanding immunopathological fallout of hu...   \n",
      "5  The hidden dangers of staying home: a London t...   \n",
      "\n",
      "                                            keywords  \n",
      "0  [COVID-19, Prevention and control, children, s...  \n",
      "1             [beliefs, health promotion, mortality]  \n",
      "2  [COVID-19, autoantibodies, cytokine storm, hyd...  \n",
      "5  [Accidental injury, COVID-19, Injuries, Lockdo...  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "curated_pmids = read_csv(os.path.join(DATAPATH,'pmids_for_training.tsv'),sep='\\t',header=0,index_col=0)\n",
    "i=0\n",
    "new_curated_pmids = []\n",
    "while i < len(curated_pmids):\n",
    "    matching_pmids = []\n",
    "    topicCat = curated_pmids.iloc[i]['topicCategory']\n",
    "    category = curated_pmids.iloc[i]['category']\n",
    "    searchterm_split = curated_pmids.iloc[i]['search terms'].split(',')\n",
    "    search_terms = [x.strip() for x in searchterm_split]\n",
    "    for eachterm in search_terms:\n",
    "        tmpdf = keywordsdf.loc[keywordsdf['keywords'].astype(str).str.lower().str.contains(eachterm)]\n",
    "        pmids = tmpdf['_id'].unique().tolist()\n",
    "        matching_pmids = list(set(pmids).union(set(matching_pmids)))\n",
    "    new_curated_pmids.append({'topicCategory':topicCat,'category':category,\n",
    "                              'description':curated_pmids.iloc[i]['description'],\n",
    "                              'subcategory':curated_pmids.iloc[i]['subcategory'],\n",
    "                              'search terms':search_terms,'matching_pmids':matching_pmids,\n",
    "                              'no of samples':len(matching_pmids)})\n",
    "    i=i+1\n",
    "new_curated_pmids_df = pd.DataFrame(new_curated_pmids) \n",
    "new_curated_pmids_df.to_csv(os.path.join(DATAPATH,'updated_pmids_for_training.tsv'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATAPATH,'updated_pmids_for_training.pickle'),\"wb\") as dumpfile:\n",
    "    pickle.dump(new_curated_pmids_df,dumpfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use keyword-mapped text to identify/expand dataset for additional search terms\n",
    "The curated dumpfile is based on keyword mapping--ie., the author categorized their papers based on keywords and those keywords could readily be mapped to a specific topicCategory. Now, pull the keywords for successfully mapped papers and  run a frequency analysis to see if additional keywords can be identified to grow the dataset.\n",
    "\n",
    "Given the large number of subtopic categories, the keywords for each category will not be inspected exhaustively, rather the 1 to 10% most frequent keywords for eachtopic will be inspected for inclusion/exclusion based on specificity/broadness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATAPATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0973aee831c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATAPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'updated_pmids_for_training.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mnew_curated_pmids_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DATAPATH' is not defined"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATAPATH,'updated_pmids_for_training.pickle'),\"rb\") as infile:\n",
    "    new_curated_pmids_df = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 570 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "FREQPATH = os.path.join(RESULTSPATH,'wordfrequencies/')\n",
    "\n",
    "for index,row in subcats.iterrows():\n",
    "    topicCategory = row['topicCategory'].replace(' / ','-').replace('/','-')\n",
    "    checkdf = keywordsdf.loc[keywordsdf['_id'].isin(row['matching_pmids'])]\n",
    "    allkw = checkdf[['_id','keywords']].explode('keywords').copy()\n",
    "    kwfreq = allkw.groupby('keywords').size().reset_index(name='counts')\n",
    "    kwfreq.sort_values('counts',ascending=False,inplace=True)\n",
    "    onepercent = kwfreq.head(n=int(round(len(kwfreq)*0.01,0)))\n",
    "    fivepercent = kwfreq.head(n=int(round(len(kwfreq)*0.05,0)))\n",
    "    tenpercent = kwfreq.head(n=int(round(len(kwfreq)*0.1,0)))\n",
    "    onepercent.to_csv(os.path.join(FREQPATH,topicCategory+'_1_percent_most_freq_kw.txt'),sep='\\t',header=True)\n",
    "    fivepercent.to_csv(os.path.join(FREQPATH,topicCategory+'_5_percent_most_freq_kw.txt'),sep='\\t',header=True)\n",
    "    tenpercent.to_csv(os.path.join(FREQPATH,topicCategory+'_10_percent_most_freq_kw.txt'),sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply search terms to LitCovid and our own API (elastic search) \n",
    "Use these searches to determine how well the terms broaden or narrow the training datasets. Use subtopic to litcovid/offtopic mapping and litcovid (to our api results) matching to improve the quality of the training datasets\n",
    "\n",
    "More advanced searches can be conducted on litcovid to limit the search of a keyword for a specific topicCategory to a particular LitCovid topic\n",
    "\n",
    "Eg- https://www.ncbi.nlm.nih.gov/research/coronavirus/docsum?text=pathology&filters=topics.Diagnosis&sort=score%20desc&page=1\n",
    "\n",
    "For certain categories, exclusion criteria may be needed. For example, the broad category, 'clinical' should encompass clinical studies which often (but not always overlap with 'prevention' and 'treatment' in LitCovid\n",
    "\n",
    "Additionally, it may make sense to include certain categories.  For example in LitCovid, 'prevention' encompasses palliative care and intubation (as these two prevent suffering and premature death respectively), however, we would consider both of these medical care for the treatment of infected individuals\n",
    "\n",
    "Litcovid categorical overlaps to consider:\n",
    "* Prevention and Treatment\n",
    "* Treatment and Case Descriptions\n",
    "* Prevention and Case Descriptions\n",
    "* Prevention and Transmission\n",
    "* Mechanism and Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maintopics = ['Diagnosis',\n",
    "              'Epidemiology',\n",
    "              'Mechanism',\n",
    "              'Prevention',\n",
    "              'Transmission',\n",
    "              'Treatment']\n",
    "\n",
    "\n",
    "## Note that Clinical is excluded from the main topics, \n",
    "## since the subtopics for this one are trained and treated like a broad topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convalescent plasma :  1145\n",
      "therapeutic plasma :  61\n",
      "neutralizing antibodies :  1224\n",
      "plasmapheresis :  69\n",
      "antibodies for treating :  2\n",
      "antibodies used to treat :  4\n",
      "antibody therapeutics :  42\n",
      "therapeutic antibodies :  82\n",
      "antibody therapies :  45\n",
      "plasma therapy :  315\n",
      "plasma exchange :  115\n",
      "immunotherapy :  801\n",
      "neutralizing antibody :  697\n",
      "antibody neutralization :  56\n",
      "passive immunization :  60\n",
      "tocilizumab :  1065\n",
      "hospital care :  259\n",
      "prehospital care :  22\n",
      "standard of care :  545\n",
      "medical care :  948\n",
      "supportive care :  555\n",
      "palliative care :  743\n",
      "terminal care :  12\n",
      "psychological care :  46\n",
      "icu treatment :  69\n",
      "intubation :  1485\n",
      "keywords :  772\n",
      "drug discovery :  816\n",
      "high throughput screening :  142\n",
      "natural products discovery :  1\n",
      "molecular docking :  1004\n",
      "molecular dynamics :  949\n",
      "antiviral virtual screening :  0\n",
      "drug design :  402\n",
      "drug development :  575\n",
      "drug delivery :  250\n",
      "drug target :  303\n",
      "natural products :  291\n",
      "drug screening :  132\n",
      "drug safety :  130\n",
      "adverse drug reactions :  105\n",
      "prognostic :  1543\n",
      "prognosis :  3130\n",
      "drug repurposing :  702\n",
      "drug repositioning :  130\n",
      "remdesivir :  1548\n",
      "FDA approved drugs :  233\n",
      "hydroxychloroquine :  2565\n",
      "doxycycline :  88\n",
      "off-label :  230\n",
      "drug combination :  57\n",
      "keywords :  772\n",
      "vaccine :  8694\n",
      "vaccination :  4284\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = 'data/'\n",
    "topic = maintopics[5]\n",
    "keyword_dict = load_search_terms(DATAPATH,topic)\n",
    "for eachkeyword in keyword_dict:\n",
    "    querylist = keyword_dict[eachkeyword]\n",
    "    for query in querylist:\n",
    "        pubmeta = requests.get('https://api.outbreak.info/resources/query?q=((\"'+query+'\") AND (@type:Publication))&size=0&aggs=@type')\n",
    "        pubjson = json.loads(pubmeta.text)\n",
    "        pubcount = int(pubjson[\"facets\"][\"@type\"][\"total\"])\n",
    "        print(query,\": \",pubcount)\n",
    "#idcheck = sub_category_id_check(DATAPATH,topic,source='litcovid')\n",
    "#for topic in maintopics:\n",
    "#    print(topic)\n",
    "#    idcheck = sub_category_id_check(DATAPATH,topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions for building the non-litcovid topics training set from keywords\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "#### Pull ids from a json file\n",
    "from src.common import get_ids_from_json\n",
    "from src.fetch_offtopics import *\n",
    "\n",
    "\n",
    "def get_subpath(DATAPATH,topic):\n",
    "    keystring = 'keywords/'+topic+'/'\n",
    "    SUBPATH = os.path.join(DATAPATH,keystring)\n",
    "    SUBFILES = os.listdir(SUBPATH)\n",
    "    return(SUBPATH,SUBFILES)\n",
    "\n",
    "\n",
    "def load_search_terms(DATAPATH,topic):\n",
    "    SUBPATH,SUBFILES = get_subpath(DATAPATH,topic)\n",
    "    keyword_dict = {}\n",
    "    for eachfile in SUBFILES:\n",
    "        filename = eachfile.split('.')[0]\n",
    "        keywords = []\n",
    "        with open(os.path.join(SUBPATH,eachfile),'r') as readfile:\n",
    "            for eachline in readfile:\n",
    "                keywords.append(eachline.strip())\n",
    "        keyword_dict[filename]=keywords\n",
    "    return(keyword_dict)\n",
    "\n",
    "\n",
    "\n",
    "## Search litcovid for a term and retrieve the pmids\n",
    "def search_litcovid_ids(searchterm,topic):\n",
    "    baseurl = 'https://www.ncbi.nlm.nih.gov/research/coronavirus-api/export/tsv?text=\"'\n",
    "    filterurl = '\"&filters={\"topics\":[\"'\n",
    "    urlend = '\"]}'\n",
    "    nofilter = '\"&filters={}'\n",
    "    if topic==False:\n",
    "        litsearchurl = baseurl+searchterm+nofilter\n",
    "    else:\n",
    "        litsearchurl = baseurl+searchterm+filterurl+topic+urlend\n",
    "    check_litcovid = requests.get(litsearchurl)\n",
    "    litcovid_data = check_litcovid.text.split('\\n')[34:]\n",
    "    pmids = []\n",
    "    for line in litcovid_data:\n",
    "        if line.startswith('#') or line.startswith('p'):\n",
    "            continue\n",
    "        pmids.append(line.split('\\t')[0])\n",
    "    cleanpmids = ['pmid'+x for x in pmids if x != \"\"]\n",
    "    return(cleanpmids)\n",
    "\n",
    "\n",
    "## load the list of search terms for each topicCategory and push it to either outbreak (default) or litcovid\n",
    "## returns an id list\n",
    "def sub_category_id_check(DATAPATH,topic,source='outbreak'):\n",
    "    mapped_topics = {'Treatment':['Prevention','Treatment','Case Report'],\n",
    "                     'Transmission':['Transmission','Prevention'],\n",
    "                     'Prevention':['Prevention','Case Report']}\n",
    "    special_cases = {'Mechanism of Transmission':['Mechanism','Transmission']}\n",
    "    keyword_dict = load_search_terms(DATAPATH,topic)\n",
    "    allids = []\n",
    "    if source == 'litcovid':\n",
    "        for category in keyword_dict.keys():\n",
    "            keywordlist = keyword_dict[category]\n",
    "            if category=='Mechanism of Transmission':\n",
    "                litcovidtopics = read_csv(os.path.join(DATAPATH,'litcovidtopics.tsv'),delimiter='\\t',index_col=0,header=0)\n",
    "                mechtrans = litcovidtopics.loc[(litcovidtopics['topicCategory']=='Mechanism')|\n",
    "                                               (litcovidtopics['topicCategory']=='Transmission')].copy()\n",
    "                mechtrans.drop_duplicates(keep='first',inplace=True)\n",
    "                freqs = mechtrans.groupby('_id').size().reset_index(name='counts')\n",
    "                meetsreqs = freqs['_id'].loc[freqs['counts']>1].unique().tolist()\n",
    "                for category in keyword_dict.keys():  \n",
    "                    keywordlist = keyword_dict[category]\n",
    "                    for eachkey in keywordlist:\n",
    "                        idlist = search_litcovid_ids(eachkey,topic)\n",
    "                        totalids = list(set(idlist).union(set(meetsreqs)))\n",
    "                        allids.append({'category':category,'searchterm':eachkey,'ids':totalids})                    \n",
    "            else:\n",
    "                if topic == 'Epidemiology':\n",
    "                    topic = False\n",
    "                    for eachkey in keywordlist:\n",
    "                        idlist = search_litcovid_ids(eachkey,topic)\n",
    "                        allids.append({'category':category,'searchterm':eachkey,'ids':idlist})                    \n",
    "                elif topic in (mapped_topics.keys()):\n",
    "                    topic_sublist = mapped_topics[topic]\n",
    "                    for eachkey in keywordlist:\n",
    "                        totalids = []\n",
    "                        for eachtopic in topic_sublist:\n",
    "                            idlist = search_litcovid_ids(eachkey,eachtopic)\n",
    "                            totalids = list(set(idlist).union(set(totalids)))\n",
    "                        allids.append({'category':category,'searchterm':eachkey,'ids':totalids})\n",
    "                else:\n",
    "                    for eachkey in keywordlist:\n",
    "                        idlist = search_litcovid_ids(eachkey,topic)\n",
    "                        allids.append({'category':category,'searchterm':eachkey,'ids':idlist})\n",
    "        idcheck = pd.DataFrame(allids)\n",
    "    else:\n",
    "        for category in keyword_dict.keys():  \n",
    "            keywordlist = keyword_dict[category]\n",
    "            for eachkey in keywordlist:\n",
    "                idlist = get_query_ids(eachkey)\n",
    "                allids.append({'category':category,'searchterm':eachkey,'ids':idlist})\n",
    "        idcheck = pd.DataFrame(allids)\n",
    "    return(idcheck)\n",
    "\n",
    "\n",
    "## Pull the id lists after search outbreak and litcovid and compare them\n",
    "## keep only ids in common for training purposes\n",
    "## Note, this will remove all preprints from the training set since litcovid does not have them\n",
    "def get_in_common_sub_ids(DATAPATH):\n",
    "    maintopics = ['Diagnosis',\n",
    "              'Epidemiology',\n",
    "              'Mechanism',\n",
    "              'Prevention',\n",
    "              'Transmission',\n",
    "              'Treatment']\n",
    "    cleandf = pd.DataFrame(columns=['category','searchterm','len_ids_x','len_ids_y','len_clean_ids','clean_ids'])\n",
    "    for topic in maintopics:\n",
    "        outbreakids = sub_category_id_check(DATAPATH,topic)\n",
    "        litcovidids = sub_category_id_check(DATAPATH,topic,source='litcovid')\n",
    "        mergedf = outbreakids.merge(litcovidids,on=(['category','searchterm']),how='outer')\n",
    "        mergedf['clean_ids'] = mergedf.apply(lambda row: list(set(row['ids_x']).intersection(set(row['ids_y']))),axis=1)\n",
    "        mergedf['len_clean_ids'] = mergedf['clean_ids'].str.len()\n",
    "        mergedf['len_ids_x'] = mergedf['ids_x'].str.len()\n",
    "        mergedf['len_ids_y'] = mergedf['ids_y'].str.len()\n",
    "        mergedf.drop(columns=['ids_x','ids_y'],inplace=True)\n",
    "        cleandf = pd.concat((cleandf,mergedf),ignore_index=True)\n",
    "    return(cleandf)\n",
    "\n",
    "\n",
    "def generate_subtraining_dict(DATAPATH,RESULTSPATH,savefile = False):\n",
    "    cleandf = get_in_common_sub_ids(DATAPATH)\n",
    "    training_dict = {}\n",
    "    for eachcat in cleandf['category'].unique().tolist(): \n",
    "        j=0\n",
    "        tmpdf = cleandf.loc[cleandf['category']==eachcat]\n",
    "        allids = []\n",
    "        while j<len(tmpdf):\n",
    "            allids = list(set(allids).union(set(tmpdf.iloc[j]['clean_ids'])))\n",
    "            j=j+1\n",
    "        training_dict[eachcat]=allids\n",
    "    if savefile == False:\n",
    "        return(training_dict)\n",
    "    else:\n",
    "        with open(os.path.join(RESULTSPATH,\"training_dict.json\"), \"w\") as outfile: \n",
    "            json.dump(training_dict, outfile)        \n",
    "\n",
    "\n",
    "def get_sub_topics(DATAPATH,RESULTSPATH):\n",
    "    training_dict = generate_subtraining_dict(DATAPATH,RESULTSPATH)\n",
    "    trainingdf = transform_training_dict(training_dict)\n",
    "    trainingdf.to_csv(os.path.join(DATAPATH,'subtopics.tsv'),sep='\\t',header=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATAPATH = 'data/'\n",
    "RESULTSPATH = 'results/'\n",
    "get_sub_topics(DATAPATH,RESULTSPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
